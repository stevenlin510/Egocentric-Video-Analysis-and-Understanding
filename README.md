# Egocentric-Video-Analysis-and-Understanding

- **An Outlook into the Future of Egocentric Vision** [[link](https://arxiv.org/pdf/2308.07123.pdf)]
***
## Dataset
### 2022
> 1. **(CVPR) Ego4D: Around the World in 3000 Hours of Egocentric Video** [[link](https://arxiv.org/pdf/2110.07058.pdf)]
> 2. **(ECCV) Find-Grained Egocentric Hand-Object Segmentation: Dataset, Model, and Applications** [[link](https://arxiv.org/pdf/2208.03826.pdf)]

### 2023
> 1. **(NeurIPS) EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset** [[link](https://proceedings.neurips.cc/paper_files/paper/2023/file/ef01d91aa87e7701aa9c8dc66a2d5bdb-Paper-Datasets_and_Benchmarks.pdf)]
> 2. **(NeurIPS) EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding** [[link](https://proceedings.neurips.cc/paper_files/paper/2023/file/90ce332aff156b910b002ce4e6880dec-Paper-Datasets_and_Benchmarks.pdf)]
> 3. **(NeurIPS) Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities** [[link](https://proceedings.neurips.cc/paper_files/paper/2023/file/7a65606fa1a6849450550325832036e5-Paper-Datasets_and_Benchmarks.pdf)]
> 4. **(ICCV) HoloAssist: An Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World** [[link](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.pdf)]
> 5. **(ICCV) Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception** [[link](https://openaccess.thecvf.com/content/ICCV2023/papers/Pan_Aria_Digital_Twin_A_New_Benchmark_Dataset_for_Egocentric_3D_ICCV_2023_paper.pdf)]
> 6. **(CVPR) AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation** [[link](https://openaccess.thecvf.com/content/CVPR2023/papers/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.pdf)]
> 7. **MIMIC-IT: Multi-Modal In-Context Instruction Tuning** [[link](https://arxiv.org/pdf/2306.05425.pdf)]

## Video-Language Pretraining Model
### 2022

> 1. **(NeurIPS) Egocentric Video-Language Pretraining** [[link](https://proceedings.neurips.cc/paper_files/paper/2022/file/31fb284a0aaaad837d2930a610cd5e50-Paper-Conference.pdf)]

### 2023

> 1. **(ICCV) EgoVLPv2: Egocentric Video-Language Pretraining with Fusion in the BackBone** [[link](https://openaccess.thecvf.com/content/ICCV2023/papers/Pramanick_EgoVLPv2_Egocentric_Video-Language_Pre-training_with_Fusion_in_the_Backbone_ICCV_2023_paper.pdf)]
> 2. **(ICCV) Helping Hands: An Object-Aware Ego-centric Video Recognition Model** [[link](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Helping_Hands_An_Object-Aware_Ego-Centric_Video_Recognition_Model_ICCV_2023_paper.pdf)]

## Visual Representation (ego-exo)

### 2021

> 1. **(CVPR) Ego-Exo: Transferring Visual Representations from Third-person to First-person Videos** [[link](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Ego-Exo_Transferring_Visual_Representations_From_Third-Person_to_First-Person_Videos_CVPR_2021_paper.pdf)]

### 2023

> 1. **(NeurIPS) Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment** [[link](https://proceedings.neurips.cc/paper_files/paper/2023/file/a845fdc3f87751710218718adb634fe7-Paper-Conference.pdf)]

> 2. **(ACMMM) POV: Prompt-Oriented View-Agnostic Learning for Egocentric Hand-Object Interaction in the Multi-view World** [[link](https://dl.acm.org/doi/pdf/10.1145/3581783.3612484)]

## Audio-Visual Object Localization
### 2023

> 1. **(CVPR) Egocentric Audio-Visual Object Localization** [[link](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Egocentric_Audio-Visual_Object_Localization_CVPR_2023_paper.pdf)]

## Object Detection
### 2023

> 1. (ICCV) Self-Supervised Object Detection from Egocentric Videos [[link](https://openaccess.thecvf.com/content/ICCV2023/papers/Akiva_Self-Supervised_Object_Detection_from_Egocentric_Videos_ICCV_2023_paper.pdf)]

## Action Detection and Recognition
### 2023

> 1. **(ICCV) Ego-only: Egocentric Action Detection without Exocentric Transferring** [[link](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Ego-Only_Egocentric_Action_Detection_without_Exocentric_Transferring_ICCV_2023_paper.pdf)]

## Egocentric Video Captioning

### 2024
> 1. **Retrieval-Augmented Egocentric Video Captioning** [[link](https://arxiv.org/pdf/2401.00789.pdf)]

## Efficient Egocentric Video Understanding
### 2023
> 1. **(NeurIPS)EgoDistill: Egocentric Head Motion Distillation for Efficient Video Understanding** [[link](https://arxiv.org/abs/2301.02217)]

## Procedure Learning
### 2022
> 1. **(ECCV) My View is the Best View: Procedure Learning from Egocentric Videos** [[link](https://arxiv.org/pdf/2207.10883.pdf)]

## Popular Model Architectures
### `2D`
1. (CVPR 2020) GSM - *Gate-Shift Networks for Video Action Recognition* [[link](https://openaccess.thecvf.com/content_CVPR_2020/papers/Sudhakaran_Gate-Shift_Networks_for_Video_Action_Recognition_CVPR_2020_paper.pdf)] 
2. (ICCV 2019) TSM - *Temporal Shift Module for Efficient Video Understanding*[[link](https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.pdf)]
3. (ICCV 2019) TBN - *EPIC-Fusion: Audio-Visual Temporal Binding
for Egocentric Action Recognition* [[link](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf)]

### `3D`
1. (ICCV 2019) SlowFast - *SlowFast Networks for Video Recognition* [[link](https://openaccess.thecvf.com/content_ICCV_2019/papers/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.pdf)]

### `Transformer`
1. (CVPRW 2022) Ego-Stan - *Building Spatio-temporal Transformers for Egocentric 3D Pose Estimation* [[link](https://arxiv.org/pdf/2206.04785)]
2. (NeurIPS 2021) XViT - *Space-time Mixing Attention for Video Transformer*[[link](https://proceedings.neurips.cc/paper/2021/file/a34bacf839b923770b2c360eefa26748-Paper.pdf)]
3. (ICML 2021) TimeSformer - *Is Space-Time Attention All You Need for Video Understanding?* [[link](https://arxiv.org/pdf/2102.05095)]
4. (ICCV 2021) ViViT - *ViViT: A Video Vision Transformer* [[link](https://openaccess.thecvf.com/content/ICCV2021/papers/Arnab_ViViT_A_Video_Vision_Transformer_ICCV_2021_paper.pdf)]

### Benchmarks
1. **Keystep (Ego-exo)**
    - Fine-grained Keystep Recognition [[link](https://docs.ego-exo4d-data.org/benchmarks/keystep/keystep_recoginition/)]

        ```
        [Train]
        Input: 1 ego + N exo trimmed video clips
        Output: Keystep label

        [Inference]
        Input: A trimmed egocentric video clip
        Output: Predicted keystep label
        ```
    - Task Graph [[link](https://docs.ego-exo4d-data.org/benchmarks/keystep/task_graph/)]
        > Determine how each task should be performed (using keysteps) based on a given video segment.
        Given a video segment $s_i$ and its segment history $S_{:i-1} = \{s_1, \ldots, s_{i-1}\}$, models have to:

        - Determine the list of previous keysteps to be performed before $s_i$;
        - Infer if $s_i$ is an optional keystep, i.e., the procedure can be completed even skipping this keystep;
        - Infer if $s_i$ is a procedural mistake, i.e., a mistake due to incorrect keystep ordering;
        - Predict a list of missing keysteps. These are keysteps which should have been performed before $s_i$ but have not been performed;
        - Forecast next keysteps. These are keysteps for which dependencies are satisfied after the execution of $s_i$ and hence can be executed next.

    - Energy Efficient [[link](https://docs.ego-exo4d-data.org/benchmarks/keystep/energy_efficient/)]
        ```
        [Input]
        - Egocentric video of arbitrary length T comprising a stream of K different sensory modalities (e.g., RGB images, audio, etc.)
        - Energy budget

        [Output]
        - Per-frame keystep label (the prediction happens at 5 fps)
        - Estimated inference energy consumption
        ```

2. **Ego-exo relations**
    - Correspondence [[link](https://docs.ego-exo4d-data.org/benchmarks/relations/correspondence/)]

        ```
        [Input]
        - Time-synchronized Egocentric + Exocentric video clips
        - Object segmentation track in Egocentric or Exocentric view

        [Output]
        Segmentation masks in the other view of the frames that the object is visible in both views. 
        ```
    - Translation [[link](https://docs.ego-exo4d-data.org/benchmarks/relations/translation/)]

3. **Proficiency Estimation (Ego-exo)** [[link](https://docs.ego-exo4d-data.org/benchmarks/proficiency_estimation/)]
    - Demonstrator proficiency estimation: the goal is to estimate the absolute skill level of a participant at the task.
        ```
        [Input]
            - Egocentric video clip
            - [Optional] Exocentric videos synchronized in timestamp

        [Output]
            Proficiency label: Novice, Early Expert, Intermediate Expert, Late Expert
        ```
    - Demonstration proficiency estimation: the goal is to perform fine-grained analysis of a given task execution to identify good actions from the participant and suggest tips for improvement. 
        ```
        [Input]
            - Egocentric video clip
            - [Optional] Exocentric videos synchronized in timestamp

        [Output]
            - Temporal localization of a proficiency category: list of tuples, each containing a timestamp, a proficiency category (i.e., good execution or needs improvement), and its probability
        ```
4. **Goal-Step (Ego)** [[link](https://github.com/facebookresearch/ego4d-goalstep)]
    - annotation
        ```
        - video_uid: unique video ID
        - start_time: A timestamp where a goal segment starts (in seconds)
        - end_time: A timestamp where a goal segment ends (in seconds)
        - goal_category: Goal category name
        - goal_description: Natural language description of the goal
        - goal_wikihow_url: A wikiHow URL that best captures the steps captured in the video
        - summary: A list of natural language descriptions summarizing steps captured in the video
        - is_procedural: Binary flag indicating whether the current segment contains procedural steps
        - segments: A list of step segments
        - start_time: A timestamp where a step segment starts (in seconds)
        - end_time: A timestamp where a step segment ends (in seconds)
        - step_category: Step category name (shares the same taxonomy with substep categegories)
        - step_description: Natural language description of the step
        - is_continued: Binary flag indicating whether the current segment contains a step that is continued from an earlier segment
        - is_procedural: Binary flag indicating whether the current segment contains procedural steps
        - is_relevant: A flag indicating whether the current segment is essential, optional, or irrelevant to the (parent) goal segment
        - summary: A list of natural language descriptions summarizing substeps captured in the video
            - segments: A list of substep segments
        ```
    



